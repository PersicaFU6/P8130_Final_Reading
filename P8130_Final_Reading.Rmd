---
title: "P8130_Final_Reading"
author: "Yujing FU"
date: "2024-12-18"
output: html_document
---
```{r message=FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(caret)
```

```{r}
# import dataset (whole data)
school <- read.csv("Project_1_data.csv", na = c("NA", ",", "")) |>
  janitor::clean_names() |>
  drop_na()

# dataset for math score
math <- school |>
  select(-reading_score, -writing_score)

# dataset for reading score
reading <- school |>
  select(-math_score, -writing_score)

# dataset for writing score
writing <- school |>
  select(-math_score, -reading_score)
```

```{r}
# whole school dataset transformed to indicator 0, 1, 2, 3, ...
school_transformed <- school |>
  mutate(across(everything(), as.factor)) |>
  mutate(across(where(is.factor), ~ as.integer(as.factor(.x)) - 1))

# whole school dataset transformed to indicator only containing 0 and 1
indicator_school <- as.data.frame(model.matrix(~ . - 1, data = school))

# math score indicator dataset
indicator_math <- indicator_school |>
  select(-reading_score, -writing_score)

# reading score indicator dataset
indicator_reading <- indicator_school |>
  select(-math_score, -writing_score)

# writing score indicator dataset
indicator_writing <- indicator_school |>
  select(-math_score, -reading_score)
```





```{r}
# model for reading_score
excluded_vars <- c("writing_score", "math_score")
predictors <- school[, !names(school) %in% excluded_vars]

# Define the response variable and the predictors
response <- school$reading_score
predictors <- predictors[, !names(predictors) %in% "reading_score"]

# Generate a formula with interaction terms for all pairwise combinations of predictors
predictor_names <- colnames(predictors)
interaction_terms <- paste(predictor_names, collapse = " + ")
formula <- as.formula(paste("reading_score ~ (", interaction_terms, ")^2"))

# Fit the linear regression model
model <- lm(formula, data = school)
summary(model)
```
From this overall linear regression model, we notice there are some significant covariates such as ??

```{r}
lm(reading_score ~ . + parent_educ*parent_marital_status+ lunch_type*practice_sport
, data = reading) |>
  summary()
```



```{r}
set.seed(1111)
model1 <- lm(reading_score ~ ., data = reading)

model2 <- lm(reading_score ~ . + parent_educ*parent_marital_status + lunch_type*practice_sport, data = reading)

anova_results <- anova(model1, model2)

print(anova_results)
```
Based on the significance condition of overall regression, we select two interaction term: `parent_educ*parent_marital_status` and `lunch_type*practice_sport`. Then we apply ANOVA to test whether adding the interaction term is better. The ANOVA P value = 0.04374 less than 0.05, indicating that the model with the addition of interaction terms significantly improves the fitting effect.

```{r}
# check the Multicollinearity
library(car)
vif(model2)
# all GVIF^(1/(2*Df)) are smaller than 5
```




```{r}
library(car)

# 1. Linearity and Homoscedasticity
par(mfrow = c(2, 2))
plot(model2)
  
# 2. Normality of Residuals
shapiro_test <-
  shapiro.test(residuals(model2))
shapiro_test
  
# 3. Independence (Durbin-Watson test)
dw_test <- durbinWatsonTest(model2)
dw_test
  
# 4. No Multicollinearity
vif_values <- vif(model2)
vif_values
```
```{r}
# Box-Cox
library(MASS)
boxcox_result <- boxcox(model2, lambda = seq(-2, 2, 0.1))

optimal_lambda <- boxcox_result$x[which.max(boxcox_result$y)]
cat("Optimal Lambda:", optimal_lambda, "\n")
```
```{r}
# Transform Y based on the optimal lambda = 1.434343
reading_trans <- reading |>
  mutate(reading_score_trans = (reading_score^optimal_lambda - 1) / optimal_lambda)


# check normality
hist(reading_trans$reading_score_trans, main = "Transformed Reading Score", xlab = "Transformed Score")

```
```{r}
library(dplyr)


# Remove the original `reading_score` column
reading_trans <- 
  reading_trans %>% dplyr::select(-reading_score)

# Fit the transformed model
model3 <- lm(reading_score_trans ~ . + parent_educ * parent_marital_status + lunch_type * practice_sport, data = reading_trans)

# Display the summary of the model
summary(model3) # improve in adjusted R2
```


```{r}
# 1. Linearity and Homoscedasticity
par(mfrow = c(2, 2))
plot(model3)
  
# 2. Normality of Residuals
shapiro_test <-
  shapiro.test(residuals(model3))
shapiro_test
  
# 3. Independence (Durbin-Watson test)
dw_test <- durbinWatsonTest(model3)
dw_test
  
# 4. No Multicollinearity
vif_values <- vif(model3)
vif_values
```



```{r, message = FALSE, warning=FALSE}
# Automatic Subset Selection Procedures based on Model3
library(leaps)
library(dplyr)
library(tidyverse)

# backward elimination
backward_model <- step(model3, direction = "backward")

# Stepwise Regression
stepwise_model <- step(model3, direction = "both")

# Summaries of the models
summary(backward_model)
summary(stepwise_model)
```


```{r}
library(leaps)
library(knitr)

# all subset regression
subset_selection <- regsubsets(
  reading_score_trans ~ . + parent_educ * parent_marital_status + lunch_type * practice_sport,
  data = reading_trans,
  nvmax = NULL
)

# summary of subsets
subset_sum <- summary(subset_selection)

# criterions
best_models <- data.frame(
  R2 = subset_sum$rsq,
  Adjusted_R2 = subset_sum$adjr2,
  Cp = subset_sum$cp,
  BIC = subset_sum$bic
) |> knitr::kable()


# print out best models
best_adjr2 <- which.max(subset_sum$adjr2)
best_cp <- which.min(subset_sum$cp)
best_bic <- which.min(subset_sum$bic)

best_model_adjr2 <- names(which(subset_sum$which[best_adjr2, -1]))
best_model_cp <- names(which(subset_sum$which[best_cp, -1]))
best_model_bic <- names(which(subset_sum$which[best_bic, -1]))

cat("Best model based on Adjusted R2 is:\n")
print(best_model_adjr2)
cat("Best model based on Cp is:\n")
print(best_model_cp)
cat("Best model based on BIC is:\n")
print(best_model_bic)

```

```{r}
# print models
model_cp <- lm(reading_score_trans ~ gender + ethnic_group + parent_educ + lunch_type + test_prep + parent_marital_status + wkly_study_hours + parent_educ:parent_marital_status + lunch_type:practice_sport, data = reading_trans)
summary(model_cp)

model_bic <- lm(reading_score_trans ~ gender + ethnic_group + parent_educ + lunch_type + test_prep + parent_educ:parent_marital_status, data = reading_trans)
summary(model_bic)
```


```{r, message = FALSE, warning=FALSE}
# LASSO
library(glmnet)

# Create the design matrix for the predictors
X_matrix <- model.matrix(reading_score_trans ~ . + parent_educ * parent_marital_status + lunch_type * practice_sport, data = reading_trans)

# Create the response vector
y_vector <- reading_trans$reading_score_trans

# Perform cross-validated LASSO
lasso_cv <- cv.glmnet(X_matrix, y_vector, alpha = 1)

# Get the best lambda value
best_lambda <- lasso_cv$lambda.min

# Fit LASSO with the best lambda
lasso_model <- glmnet(X_matrix, y_vector, alpha = 1, lambda = best_lambda)
```

```{r}
coefficients_matrix <- coef(lasso_model)
# Convert the coefficients matrix to a regular matrix to perform comparison
coefficients_matrix <- as.matrix(coefficients_matrix)

# Select the non-zero coefficients
non_zero_coefficients <- coefficients_matrix[coefficients_matrix != 0]

# Print non-zero coefficients
cat("Non-zero coefficients for LASSO model are:\n")
print(non_zero_coefficients)
```

```{r}
# best lasso model
model_lasso <- lm(reading_score_trans ~ gender + ethnic_group + parent_educ + lunch_type + test_prep + parent_marital_status + practice_sport + is_first_child + nr_siblings + wkly_study_hours + parent_educ:parent_marital_status + lunch_type:practice_sport, data = reading_trans)
summary(model_lasso)
```


- Test the model predictive ability using a 10-fold cross-validation.
```{r, message = FALSE, warning=FALSE}
# Load necessary package
library(caret)

# Set up the cross-validation method (10-fold cross-validation)
train_control <- trainControl(method = "cv", number = 10)

# Fit the model using 10-fold cross-validation
cv_model <- train(
  reading_score_trans ~ gender + ethnic_group + parent_educ + lunch_type + test_prep + parent_marital_status + practice_sport + is_first_child + nr_siblings + wkly_study_hours + parent_educ:parent_marital_status + lunch_type:practice_sport, 
  data = reading_trans, 
  method = "lm", 
  trControl = train_control
)


print(cv_model)

# To view the RMSE (Root Mean Squared Error) or other metrics
cv_model$results |> knitr::kable()
```




